\documentclass[reprint, amsmath, amssymb, aps, floatfix]{revtex4-1}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{bm}
\usepackage{float}
\usepackage[font=small,labelfont=bf]{caption}
\captionsetup{justification=raggedright,
	singlelinecheck=false}
\usepackage{listings}
\usepackage{siunitx}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage[english]{babel}
\usepackage[autostyle, english = american]{csquotes}
\MakeOuterQuote{"}
\usepackage{hyperref}
\hypersetup{colorlinks=true, allcolors=blue}
\def\inline{\lstinline[basicstyle=\ttfamily,keywordstyle={}]}


\usepackage{needspace}

\newcommand{\bv}[1]{\mathbf{#1}}



\begin{document}

\title{A Parallel Computational Investigation\\of Poisson's Equation}

\author{Alec Wills}
\email{alec.wills@stonybrook.edu}
\affiliation{Department of Physics and Astronomy\\Stony Brook University\\Stony Brook, NY 11794-3800, USA}
\date{May 4, 2018}

\begin{abstract}
	We investigate  parallel processing in numerically solving the Poisson equation in two dimensions. A mesh of points defining the domain of $\mathbb{R}^2$ in which we solve the equation is decomposed along one dimension and sent to different processes. These processes then relax their subdomain via Jacobi iteration and communicate results with processes carrying neighbor subdomains, so that further relaxation may occur. When relaxation yields a total error of no more than $10^{-6}$, simulation terminates and we characterize the accuracy of the numerical solution compared to the analytical, as well as investigate the timing of using multiple processes. We find that increasing the number of processes from one to multiple increases the simulation speed, but increasing beyond two does not change the simulation time by an appreciable amount.
\end{abstract}

\maketitle

\section{Introduction}

\subsection{Motivation}

Computational studies of intractable problems are becoming increasingly popular (and important) in gleaning information about the system in question. In an ever increasingly technological world, efficiency at analyzing and computing data is a key factor in the success of many fields of research as well as in general industry. For this reason, an investigation into parallel computing becomes essential in learning the basic techniques being employed by cutting edge institutions in their work with large amounts of information. In particular, almost all available molecular dynamics simulation programs (quantum or classical) necessitate the use of parallelizing their execution, given the massive number of coordinates to keep track of ($3N$ for $N$ particles and all you care about is position) and interaction terms (on the order of $N^2$ for only pairwise interactions). Many programs offer hybridized parallel processing capabilities, such as the classical simulations in GROMACS \cite{gromacs}, while others focus primarily on a single type of parallelizing, such as the quantum simulations of SIESTA \cite{siesta}.

\subsection{Parallel Computation}

As a brief introduction, we describe the general concepts involved in parallel computation, as well as justify why we chose our particular parallelization scheme in this project.

As an example, we consider two vectors $\bv{a}$ and $\bv{b}$ in $\mathbb{R}^n$. When computing the dot product $$c = \bv{a}\cdot\bv{b}=\sum_i^n a_ib_i,$$ one needn't compute the product in any particular order. Going from $i=1$ to $i=n$ in sequential order is no different than choosing a random $i$ out of remaining products, and then summing them at the end. In this sense, the dot product is very much \textit{parallelizable}, in that anybody could compute a product in the sum without needing to know any other product. At the end, the results would be communicated back together to form the sum. In a computational sense, instead of computing the dot product in order by one process many different processes could be executed, each computing a separate product and drastically reducing computation time. In this particular example, we perform the same instruction on multiple data points, and thus this falls under the Single-Instruction-Multiple-Data (SIMD) umbrella of parallel computation. Other categories exist, involving varying amounts of instructions and data, but the concept remains the same -- one breaks up a parallelizable process into independent processes and combines whatever result at the end.


\begin{figure}
\includegraphics[scale=0.5]{parallel.png}
\captionof{figure}{An example of a hybrid model of parallel processing, wherein the separate memory spaces (rectangles) have subprocesses (circles) that share the space, while communication over a network (gray blob) between the separate memory spaces doesn't allow universal access to the data being worked on. Reprinted from \cite{mpibook}.}
\label{pic}
\end{figure}

Within these umbrellas of instruction-data categories, one may further specify whether a parallel computation is through \textit{shared memory} or processes work on their own memory spaces without access to the global range of data. A diagram depicts this concept in Fig. \ref{pic}. In our example, a shared memory implementation of the dot product might compute the sums across subprocesses but add the answer to a globally available sum.

An alternative scheme, and the one used in this work, is called the \textit{message-passing} model. In this regime, memory spaces are not global. Subprocesses only have access to a determined subregion of the data, for instance indices $j$ to $k$ in our example. Here, communication must occur between the different memory spaces and the corresponding subprocesses so that other spaces are aware of what has happened, if needed. In our example, the subset of indices available to a given memory space could be summed, and then the partial sum communicated back to some managing process to form the total sum in the dot product. This concept is also depicted in Fig. \ref{pic}.

Given the customizability of the message-passing model (in how the data is subdivided), and the protection of subdomains of data, we utilize the message-passing model in our work. In particular, we use the MPICH implementation of the interface. Particular versions and compilation environments are detailed in the Methods section.



\section{Theory}

\subsection{An Analytical Solution}

We here put forth an analytical solution to the sourceless Poisson equation \begin{equation}
\nabla^2\phi(x,y)=0 \label{sourceless}
\end{equation} defined in a rectangular region $[0, a]\times [0, b]$ with boundary conditions
\begin{equation}
\begin{aligned}
\phi(0, y) &= 0 \\
\phi(a, y) &= 0\\
\phi(x, 0) &= 0\\
\phi(x, b) &= f(x) \label{bcs}
\end{aligned}
\end{equation} where the upper border's boundary condition will remain arbitrary until we specify its form. Since Eq. \ref{sourceless} is linear and homogeneous, we assume that $\phi(x,y)$ is separable such that \begin{equation} \phi(x,y)=X(x)Y(y). \label{separable} \end{equation} Now using this form of $\phi(x,y)$ and plugging into the Eq. \ref{sourceless} yields two decoupled ordinary differential equations \begin{equation}
\frac{X''(x)}{X(x)} = -\frac{Y''(y)}{Y(y)} = - \lambda, \label{separated}
\end{equation} and it is these equations with which we construct a solution. The minus sign in front of $\lambda$ is chosen for convenience. The solution to this type of differential equation is well-known, and we begin by stating the solution for $X(x)$: \begin{equation}
X(x) = A\sin\left(\frac{n\pi x}{a}\right), \label{xeq}
\end{equation} where we've enforced the boundary conditions $X(0, y)=X(a, y)=0$. This therefore determines the separation constant \begin{equation}
\lambda = \left(\frac{n\pi}{a}\right)^2 \label{lambda}
\end{equation} which we may use to solve the differential equation for $Y(y)$, subject to $Y(0)=0$ and $Y(b)=f(x)$. With the constant determined and positive, we thus have another well known studied solution \begin{equation}
Y(y) = B\sinh\left(\frac{n\pi y}{a}\right), \label{yeq}
\end{equation} where again we've imposed the grounded boundary condition to remove the $\cosh(y)$ term. We thus have a solution for $\phi(x,y)$ solving the three homogeneous boundary conditions, but have yet to fix the solution to fit the nonhomogeneous condition at $\phi(x,b)$. To do so, we use the superposition principle and dial the constants of the sum to be such that the upper boundary condition is satisfied: \begin{equation}
\phi(x,y)=\sum_{n=1}^\infty c_n \sin\left(\frac{n\pi x}{a}\right)\sinh\left(\frac{n\pi y}{a}\right) \label{phisum}.
\end{equation}

In dialing the solution, we must choose coefficients satisfying \begin{equation}
f(x)=\sum_{n=1}^\infty c_n \sin\left(\frac{n\pi x}{a}\right)\sinh\left(\frac{n\pi b}{a}\right). \label{feq}\end{equation} We note that the expansion on the right hand side is the Fourier decomposition of $f(x)$. We utilize the orthogonality of the Fourier sine functions, and take the inner product (in the integral sense) of both sides. \begin{equation}
\begin{aligned}
&\int_{0}^{a}dx\cdot f(x)\sin\left(\frac{m\pi x}{a}\right)\\&= \sum_{n=1}^\infty c_n \sinh\left(\frac{n\pi b}{a}\right)\\&\times\int_0^a dx\cdot \sin\left(\frac{n\pi x}{a}\right)\sin\left(\frac{m \pi x}{a}\right).
\end{aligned}
\end{equation} The orthogonality yields a term proportional to $\delta_{mn}$ from the integration, which collapses the summation to a single term. The proportionality factor is just $\frac{a}{2}$, as shown in Appendix A. Thus we have reached \begin{equation}
c_n = \frac{2}{a\sinh\left(\frac{n\pi b}{a}\right)} \int_{0}^{a} dx\cdot f(x)\sin\left(\frac{n\pi x}{a}\right). \label{constant}
\end{equation}

The boundary condition $f(x)$ above is completely arbitrary, but at this point one needs to specify an explicit form and construct the coefficients term by term to carry out the sum to a desired precision. We here specify that the given boundary potential used in the simulations is \begin{equation}
f(x)=5x-100 \label{fofx}
\end{equation} and state the final result for the coefficient formula, referring the reader to Appendix B for the derivation: \begin{equation}
\begin{aligned}
c_n =& \frac{2}{a\sinh\left(\frac{n\pi b}{a}\right)}\\& \times\frac{-5a}{n\pi}\left[(a-20)\cos(n\pi)+20\right]
\label{finalceq}
\end{aligned}
\end{equation} Thus we have found an analytical solution solving the Poisson equation with the specified boundary potential. This solution is shown in Fig. \ref{analytical}.

\begin{figure}
	\hspace{-1cm}
	\includegraphics[scale=0.7]{analytical.png}
	\captionof{figure}{The analytical solution to the Poisson equation with the boundary conditions of this section, summed over 500 terms. Qualitatively, one can see it fits the expected boundary conditions with slight deviations near the $x$ limits.}
	\label{analytical}
\end{figure}

Implementations of nonzero boundary terms are, of course, possible for the other sides of the region, and the above analysis extends to each side so long as the boundary conditions are linear. One may use the superposition principle to construct an arbitrary solution to boundary potentials at each side so long as the series functions are constructed individually -- that is, for each solution you can only assume one side is nonzero.

We note, finally, that if there were a source (i.e., the Poisson equation became inhomogenous), the solution would be a superposition of the homogeneous one and the particular one for the source. But we know the potential for a range of source classifications, namely a point source. In this case, one gets the solution above added to \begin{equation} V_p = \frac{Q}{4\pi\epsilon_0 r}, \label{pointv} \end{equation} where $r$ is the distance from the location of the point source.

\subsection{A Numerical Solution}

We examine the equation \begin{equation}
\nabla^2\phi(x,y)=f(x,y) \label{sourceeq}
\end{equation} and seek a solution for $\phi(x,y)$.
In doing so, along the same Cartesian region $[0,a]\times [0,b]$ as above, we discretize the domain into a mesh. We do not specify the mesh size until later, going from a general case to our specific considerations.

In order to discretize the Laplacian of our potential, on our mesh we consider a point $\phi_{ij}$, using the notation $$\phi_{ij}=\phi(x_i,y_j).$$ We Taylor expand our function one $x$-point behind $\phi_{ij}$ while keeping $y$ fixed: \begin{equation}
\begin{aligned}
\phi_{i-1,j} =&\ \phi_{ij} + \partial_x \phi_{ij}(x_i-x_{i-1})\\&+\frac{1}{2}\partial_{xx}\phi_{ij}(x_i-x_{i-1})^2, \label{taylor}
\end{aligned}
\end{equation} where we neglect cubic order terms, assuming our mesh is fine enough. We can carry out the same expansion but at the point ahead of our $x$, yielding an analogous solution with $x_{i-1}\to x_{i+1}$. We here define \begin{equation}
\begin{aligned}
\Delta x_- &= x_i-x_{i-1}\\
\Delta x_+ &= x_i-x_{i+1} \label{deltaeq}
\end{aligned}
\end{equation} for notation simplification. We can solve these two equations for $\partial_{xx}\phi_{ij}$ in terms of $\phi_{ij}$ and $\partial_x\phi_{ij}$ by adding Eq. \eqref{taylor} and the expansion for $\phi_{i+1,j}$ to yield \begin{equation}
\begin{aligned}
\partial_{xx}\phi_{ij} &= \frac{2}{\Delta x_-^2+\Delta x_+^2}\biggl[\phi_{i-1,j}+\phi_{i+1,j}-2\phi_{ij}\\&-\partial_x\phi_{ij}(\Delta x_-+\Delta x_+)\biggr], \label{xxeq}
\end{aligned}
\end{equation} where we have everything in terms of the mesh grid except for the first order derivative. But we can solve for this in terms of the grid values as well, again expanding ahead or behind of the given grid point to find \begin{equation}
\partial_x\phi_{ij}=\frac{\phi_{i+1,j}-\phi_{i-1,j}}{\Delta x_+ - \Delta x_-} \label{ptlxeq}
\end{equation} which we may use to plug into Eq. \eqref{xxeq} to solve for the second derivative solely in terms of the grid point and its neighbors. We again introduce notation, letting \begin{equation}
\begin{aligned}
\alpha_x &= \Delta x_++\Delta x_-\\
\beta_x &= \Delta x_+-\Delta x_-\\
\gamma_x^2 &= \Delta x_+^2+\Delta x_-^2 \label{params}
\end{aligned}
\end{equation} to further simplify equations.

Carrying forth the exact same analysis but with $x\to y$ and analogous definitions will finally yield a numerical form of Eq. \eqref{sourceeq}: \begin{widetext}
	\begin{equation}
	\frac{2}{\gamma_x^2}\left[\phi_{i-1,j}+\phi_{i+1,j} - 2\phi_{ij} - \left(\phi_{i+1,j}-\phi_{i-1,j}\right)\frac{\alpha_x}{\beta_x}\right] + \frac{2}{\gamma_y^2}\left[\phi_{i,j-1}+\phi_{i,j+1} - 2\phi_{ij} - \left(\phi_{i,j+1}-\phi_{i,j-1}\right)\frac{\alpha_y}{\beta_y}\right] = f_{ij} \label{numsourceeq}
	\end{equation}
	\end{widetext} where we have thus far allowed for an arbitrary spacing between the forward and backward points in each dimension.

We now consider the case where the grid is evenly spaced in its dimensions, i.e. $\Delta x_-=\Delta x_+ = \Delta x$ and similarly for $\Delta y$ (but the spacings need not be equal to each other in our case). Then, by construction $\Delta x_+=-\Delta x$ and $\Delta y_+=-\Delta y$. These assumptions yield vanishing $\alpha_x$ and $\alpha_y$ terms, whereas $\beta_x=2\Delta x$ and $\gamma_x^2 = 2\Delta x^2$. Similar definitions hold for the $y$ terms. 

Noting that we can solve for $\phi_{ij}$ as a function of its neighboring points and the source at that point, we will be able to iteratively solve the final equation \begin{widetext}
\begin{equation}
\phi_{ij} = \frac{\Delta y^2(\phi_{i-1, j} + \phi_{i+1,j}) + \Delta x^2(\phi_{i, j-1}+\phi_{i, j+1}) - (\Delta x^2+\Delta y^2)f_{ij}}{2(\Delta x^2+\Delta y^2)} \label{jitereq}
\end{equation}
\end{widetext} through successive iterations until a result of desired accuracy is achieved.

One may implement arbitrary boundary conditions here, so long as the changing iteration regions are chosen carefully and exclude the fixed borders. Sources may be defined as well, so the solution is a general numerical solution to the arbitrary potential problem.



\section{Methods}

\subsection{Computational Environment}

All simulations were coded in modern Fortran and compiled using GNU Fortran 7.2.0's compiler. The workstation on which the simulations were run is the author's personal laptop, containing four Intel\textsuperscript{\textregistered}  Core\textsuperscript{TM} i5-4200U CPUs, with a base process frequency of 1.60GHz \cite{intel}. The MPI interface used in compilation and coding was MPICH2 1.5rc3 \cite{mpich}, which, despite being outdated, was utilized primarily for its default inclusion of the MPE 2.4.6 processor timing utility \cite{mpe}. Code submitted for grading via Blackboard does not contain this timing utility due to lack of default inclusion on the Mathlab MPICH 3.0.4 versions.

Some simulations were run on Stony Brook University's Mathlab machines as the network \inline{compute.mathlab.stonybrook.edu} allowed for computation with 8 CPUs, although simulations run on these machines were much less thorough and more for benchmarking purposes. 

Plots were generated using Python 3.6.5, with the \inline{matplotlib} 2.2.2 package, and most notably its subpackage \inline{pyplot}. Some analytical work was also done using Mathematica 11.2.

\subsection{Simulations and Data Reduction}

All simulations were run using the boundary conditions of Eqs. \eqref{bcs}, using an upper boundary function as given in Eq. \eqref{fofx}. Half of the simulations were run with a source in the center of the mesh grid. The error tolerance between iterations for all simulations was $\epsilon=10^{-6}$ V, whereupon the mesh was considered relaxed after the error dropped below the tolerance. In each case (source and no source), tests were run with one through four processors on the workstation, while a benchmark test was done on the Mathlab machine using eight processors. Source code is available as listed in \cite{git}. Although the resulting output files were too large in size to upload, they are available upon request.

The final result of the iteration was output to a \inline{.dat} file, which was read into and plot in Python, resulting in the figures printed in this report. The analytical solution was created in Mathematica, as the interactive math environment was more suited to the task without importing different modules as would be necessary in Python. The MPE implementation outputs a \inline{.clog2} file, which is converted into a \inline{.slog2} format for visualizing the processing times.

\section{Analysis and Results}

As the relaxation method did not yield different answers for different processor numbers, we only show the forms of the sourceless and sourced relaxed solution here, and a typical example of the output MPE logfiles as well. Remaining logfile graphics can be seen in at the end of the document.

\subsection{Relaxation Accuracy}

We here compare our sourceless relaxed solutions to the analytical form found in the Theory section. A heatmap of our result can be seen in Fig. \ref{heatmapnosource}, while a surface plot of the relaxed solution can be seen in Fig. \ref{graphnosource}. GIFs of the relaxation process (both the heatmap and surface plot visualizations) are available in provided GitHub repository \cite{git}.

\begin{figure}
	\includegraphics[scale=0.5]{four_bc_heatmap.pdf}
	\captionof{figure}{A heatmap of the values of our relaxed potential. The boundary condition can be seen in the linear increase from black to white along the top boundary, indicating an increasing voltage.}
	\label{heatmapnosource}
\end{figure}

We compare the relaxed grid values from out potential mesh with those in Eq. \eqref{phisum}, using the substituted coefficients in Eq. \eqref{finalceq} summed to 500 terms. The maximum error value in taking the difference of these numerical and analytical solutions is $\sim 213$ volts, occurring at the edges of the boundary condition. The large error in the corners is explained by the summation error in the analytical solution -- had we summed to a larger number of terms the analytical solution would have converged closer to the limits of the $x$-axis. 


\begin{figure}
	\includegraphics[scale=0.5]{four_bc_graph.pdf}
	\captionof{figure}{The surface plot of our relaxed potential, solving the specified boundary condition.}
	\label{graphnosource}
\end{figure}


Up to the 500 terms we did, however, there is a small range of about $0.5$ cm from each border along the upper boundary where the potential sharply declines to zero to meet the left and right boundary conditions prematurely. It is in these regions that our error is prevalent. We note that taking the mean error across the difference region is a better statistic for accuracy of the relaxed result. In Tab. \ref{tab:stats} we quote the means and standard deviations in the errors of three different regions, excluding some $y$ value regions to demonstrate that the error is almost exclusively along the boundary $(x, y=15)$, indicated by the large drop in standard deviation of error values.

\begin{table}
	\vspace{2ex}
\begin{tabular}{|c|c|c|}
	\hline
	Statistic Region & Mean (V) & $\sigma$ (V)  \\ \hline
	$y\in[0,15]$ & $-0.128$ & $2.727$ \\ \hline
	$y\in[0, 14]$ & $-0.118$ & $0.319$ \\ \hline
	$y\in[0, 13]$ & $-0.110$ & $0.241$  \\ \hline
\end{tabular}
\vspace{1ex}
\captionof{table}{Statistical values referenced in the body of this section. Note the sharp decrease by an order of magnitude in $\sigma$ once we step away from the border of the boundary condition, where the convergence in the corners causes a large amount of error.}
\label{tab:stats}
\end{table}

A surface plot of the error is shown in Fig. \ref{diffmat}, further justifying the conclusion that the convergence of the analytical solution towards the corners of the region is the main source of error.

\begin{figure}
	\includegraphics[scale=0.5]{diffmat_graph.pdf}
	\captionof{figure}{A surface plot of the difference between the analytical solution, summed to 500 terms, and the relaxed solution. The majority of the grid has essentially zero error, and the bulk of the error contribution occurs in the corners where the analytical solution has not yet converged.}
	\label{diffmat}
\end{figure}


In the sourced solutions, $f(x,y)$ was set to be a point source at the middle of the grid with a strength of $ \frac{\rho}{\epsilon_0}=-100$ V. For sake of space, this heatmap and surface plot is shown at the end of the report, along with the appropriate error surface comparing the analytical solution to the relaxed grid. In it, one can see that the relaxed solution is much less singular at the point of the source. This intuitively makes sense, as the numerical solution of Eq. \eqref{jitereq} has no singularity at the point where we place the source, whereas the usual electrostatic potential Eq. \eqref{pointv} diverges as one approaches the source. Inherently, then, Jacobi iteration as used here will underestimate the potential near a small source. This is shown experimentally in our simulation by seeing the error is positive in the surface plot in Tab. \ref{simfigs} at the point source.

We further note that in all simulations, an initial potential of 50 V was applied to the grid, but this bias relaxes to zero due to the grounding of three of the sides.



\subsection{Timing}

We summarize the results of our simulation times in Tab. \ref{tab:times}, with sourced and sourceless grid relaxations. We note the approximate halving of the times from one processor to two, where it remains at a steady computation time.

\begin{table}
	\vspace{2ex}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{Processors} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4}\\ \hline
		\textbf{Sourceless Time} (s) & 66.985 & 35.235 & 41.796 & 35.724\\ \hline
		\textbf{Sourced Time} (s) & 61.741 & 34.845 & 39.814 & 36.622\\ \hline
	\end{tabular}
	\vspace{1ex}
	\captionof{table}{Relaxation times for simulations run on the main workstation. It appears as though increasing beyond one processor does not increase simulation efficiency by very much.}
	\label{tab:times}
\end{table}

We further note that running the sourceless simulations on a Mathlab machine decreases the simulation time to around 9 seconds -- a vast improvement over the times it takes on the workstation. This might be in part due to the increased processor number, although it is more likely that the increase in efficiency is due to the CPUs on the machines being much faster in general, with \inline{lscpu} denoting their processing speed around 3 GHz, almost twice as fast as the author's workstation.

MPE logging allowed for the coarse characterization of the processing times of the program amongst the different subprocesses, and an indicative graphic of the output is shown in Fig. \ref{mpe4}. Not much could be gleaned from the logfile outputs, as the \inline{jumpshot} program provided with MPE for interpreting the logs didn't allow for much interactivity. However, it is clear from the graphs that the iteration processes of the relaxation take up the vast majority of the time, nominally less than $1/p$ percent of the total computation time.

\begin{figure}
	\centering
	\includegraphics[scale=0.3]{four.png}
	\captionof{figure}{A typical output of the \inline{jumpshot} program using the MPE \inline{.slog2} logfile. Here, the blue and purple bars are the two iterative sweeps in the main program loop. The white and green bars are the ghost zone communication after the sweeps. Not visible at this scale are the instantiation processes of the Cartesian process domain, and other one-time events.}
	\label{mpe4}
\end{figure}

It is known that communication between the processes is much more time-consuming than operations on the data \cite{mpibook}, although Fig. \ref{mpe4} might suggest otherwise due to the sheer size difference between the number of operations compared to the number of communications. Each subprocess has on the order of $N_x\cdot N_y^i$, where $N_x$ is the number of bins the $x$-axis is divided into and $N_y^i$ is the number of bins along the $y$-axis apportioned to process $i$. This is compared to the communication of just $N_x$ values between processes, which can be reasonably short in comparison if the mesh spacing is rather fine.

With a communication time model given by \begin{equation}
T_c = 2\left(s+r\frac{8N_x}{p}\right),
\label{tceq}
\end{equation} where $s$ is the initialization time of the exchange of ghost zones, and $r$ is the byte-communication rate of $N_x$ 8-byte real numbers across $p$ processes (occurring twice) \cite{mpibook}, we can see that the number of processors will only help the communication time to a certain point. The operation time will scale much faster than the communication time, which is what we seem to see in our results. In fact, in the one-dimensional decomposition case that we carried out, the efficiency \begin{equation}
e = \frac{1}{1+\frac{2(s+rn)p}{fn^2}}, \label{eff}
\end{equation} where $f$ is the floating point operation time \cite{mpibook}, will increase at fixed $p$ for increasing $n$ (a given grid dimension size) and decreases at fixed $n$ for increasing $p$.

\section{Conclusion}

We have explored the utility of parallel processing, in particular the message-passing interface implementation of it. As a model study, we parallelize the Jacobi iterative solution to the Poisson model, both with and without a source. We have found that the relaxation is accurate away from divergent points (i.e., a point source) or away from where the analytical solution is not well converged. The timing of our simulation does not scale well with process number, and this is evident with the approximately constant simulation time (on the workstation) for any process number greater than one, although there is a substantial decrease in simulation time compared to a single process.

Further studies of the subject might include a two-dimensional decomposition of the process domains, or hybridizing the parallelization to include shared memory interfaces, such as OpenMP. Additionally, GPU acceleration is another avenue of interest, using tools such as CUDA on machines that allow such performance boosts.



\appendix
\section{Fourier Orthogonality}

We here prove that the Fourier components are orthogonal.

Let \begin{equation}
I = \int_{0}^a dx\cdot \sin\left(\frac{n\pi x}{a}\right)\sin\left(\frac{m\pi x}{a}\right).
\end{equation} We use the identity \begin{equation}
\sin(x)\sin(y)=\frac{1}{2}\left[\cos(x-y)-\cos(x+y)\right]
\end{equation} to rewrite and integrate forward \begin{equation}
\begin{aligned}
I &= \frac{a}{2}\biggl[\frac{1}{\pi(n-m)}\sin\left(\pi(n-m)\right)\\&-\frac{1}{\pi(n+m)}\sin\left(\pi(n+m)\right)\biggr].
\end{aligned}
\end{equation} Now, $\sin(k\pi)=0$ for all $k\in\mathbb{Z}$, so the second term vanishes without any issue (since we're considering $m,n>0$). The first term will also vanish for $n\neq m$, but we recall that \begin{equation}
\lim\limits_{x\to 0} \frac{\sin(x)}{x} =1,
\end{equation} so setting $n=m$ and letting $\pi(n-m)=x$ yields a nonvanishing first term, resulting in \begin{equation}
I=\frac{a}{2} \delta_{nm},
\end{equation} as required.

\section{Coefficient Determination}
We here want to solve the integral in Eq. \eqref{constant}, so we let \begin{equation}
K = \int_0^a dx\cdot (5x-100)\sin\left(\frac{n\pi x}{a}\right).
\end{equation} The second integral is straightforward: \begin{equation}
-100\int_0^adx \cdot \sin\left(\frac{n\pi x}{a}\right) = 100\cdot \frac{a}{n\pi}\left[\cos(n\pi)-1\right].
\end{equation}

Solving the first integral by integration by parts, \begin{equation}
\begin{aligned}
5\int_0^a dx\cdot x\sin\left(\frac{n\pi x}{a}\right) &= 5\left[\frac{-a}{n\pi}x\cos\left(\frac{n\pi x}{a}\right)\right]_0^a \\ &- 5\int_0^a dx\cdot \sin\left(\frac{n\pi x}{a}\right).
\end{aligned}
\end{equation} 

This evaluates to \begin{equation}
\begin{aligned}
5\int_0^a dx\cdot x\sin\left(\frac{n\pi x}{a}\right) &= \frac{-5a^2}{n\pi}\cos(n\pi),
\end{aligned}
\end{equation} since the final integral above results in zero.

We combine these results and factor out a $\frac{-5a}{n\pi}$ to find \begin{equation}
K = \frac{-5a}{n\pi}\left[(a-20)\cos(n\pi)+20\right].
\end{equation} Multiplying this by the prefactor derived in the text yields the coefficient result we use in our expansion.


\bibliographystyle{unsrt}%Used BibTeX style is unsrt
\bibliography{refs}


\clearpage

\begin{table*}[t]
	{\large \sc Simulation Figures.}\\
	\centering
	\begin{tabular}{cc}
		\includegraphics[scale=0.5]{four_bcs_heatmap.pdf}
		&
		\includegraphics[scale=0.5]{four_bcs_graph.pdf}\\ (a)&(b)\\
		
		\includegraphics[scale=0.5]{ptdiff_graph.pdf} 
		&
		\includegraphics[scale=0.3]{msg.png} \\ (c)
		& (d)\\
	\end{tabular}
	\captionof{table}{The figures from the remaining simulations. (a) A heatmap of the relaxed solution to the point source problem, where the center has a source creating a voltage point of $100$ V. (b) The surface plot for the heatmap. (c) The error surface plot substracting the relaxed solution's grid from the analytical solution. Note the more intense singularity in the analytical form, evident from the high error at the source point. (d) A zoomed in diagram showing step-by-step process work in a loop of the program. The white arrows indicate the communication of ghost zones between neighbor processes, and the purple and blue blocks are iterative steps.}
	\label{simfigs}
\end{table*}



\end{document}